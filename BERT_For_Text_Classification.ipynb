{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlcI1kmIEHZMI6hxnc0pGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TalCordova/PyTorch_Practice/blob/main/BERT_For_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT For Sentiment Analysis with PyTorch üêçüî•üî•"
      ],
      "metadata": {
        "id": "kzVyF50UNFn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E4pVKhi9NCMf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get ATIS Data"
      ],
      "metadata": {
        "id": "JK1xOnXxR1D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDuqKe0qR0ln",
        "outputId": "404253de-8cb3-4d99-f406-023ab9975a41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "atis_dir = '/content/drive/MyDrive/atis'"
      ],
      "metadata": {
        "id": "QmVsWNidR3SG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in os.listdir(atis_dir):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ8uCLvZSCOO",
        "outputId": "74acb44b-cf49-486b-d9d9-44c759ff4f99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slot_label.txt\n",
            "dev\n",
            "intent_label.txt\n",
            "test\n",
            "train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(directory, filename):\n",
        "    full_path = os.path.join(directory, filename)\n",
        "    with open(full_path, 'r', encoding='utf-8') as f:\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            return None\n",
        "        sentence = []\n",
        "        while line and (line != \"\\n\"):\n",
        "            line = line.strip()\n",
        "            sentence.append(line)\n",
        "            line = f.readline()\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "gGeUvarDSsRm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and labels\n",
        "train_data = read_data(atis_dir,'train/seq.in')\n",
        "train_labels = read_data(atis_dir,'train/label')\n",
        "test_data = read_data(atis_dir,'test/seq.in')\n",
        "test_labels = read_data(atis_dir,'test/label')"
      ],
      "metadata": {
        "id": "aqNOfIFYSyUG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "c650a82c-d1a9-4fef-ab6b-b10e42cecf36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-86e2a75a6e1d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matis_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train/seq.in'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matis_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train/label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matis_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test/seq.in'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matis_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test/label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ec76b3c89a51>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(directory, filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfull_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "ypJki_rkTeSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "W22FaklaUeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_items = list(set(train_labels))\n",
        "print(unique_items)"
      ],
      "metadata": {
        "id": "k0AQflKyU0Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Custom Dataset"
      ],
      "metadata": {
        "id": "DBYpOK2ByXbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ATISDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_dir, tokenizer, split = 'train', max_length = 512):\n",
        "    self.data_dir = data_dir\n",
        "    self.tokenizer = tokenizer\n",
        "    self.split = split\n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.encodings, self.labels, self.label_to_idx = self._load_and_tokenize(self.split)\n",
        "    self.num_classes = len(set(self.labels))\n",
        "\n",
        "  def _load_and_tokenize(self, split):\n",
        "    data = read_data(self.data_dir, f\"{split}/seq.in\")\n",
        "    labels = read_data(self.data_dir, f\"{split}/label\")\n",
        "\n",
        "    unique_labels = sorted(set(labels))\n",
        "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    label_indices = [label_to_idx[label] for label in labels]\n",
        "    label_tensor = torch.tensor(label_indices)\n",
        "\n",
        "    encodings = self.tokenizer(\n",
        "        data,\n",
        "        padding = 'max_length',\n",
        "        max_length = self.max_length,\n",
        "        truncation = 'longest_first',\n",
        "        return_tensors = 'pt')\n",
        "\n",
        "    return encodings, label_tensor, label_to_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "      'input_ids': self.encodings['input_ids'][idx],\n",
        "      'attention_mask': self.encodings['attention_mask'][idx],\n",
        "      'labels': self.labels[idx]\n",
        "    }"
      ],
      "metadata": {
        "id": "GoBsUkd4U53t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "train_dataset = ATISDataset(atis_dir, tokenizer, split = 'train')\n",
        "test_dataset = ATISDataset(atis_dir, tokenizer, split = 'test')"
      ],
      "metadata": {
        "id": "T0ZzHn4dvsR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "WVT7moSExJXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn into DataLoaders"
      ],
      "metadata": {
        "id": "rXgkZjUvyxEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "MeTg2POY0xMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Model"
      ],
      "metadata": {
        "id": "wqgq9Ag5ybYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_dataset.label_to_idx)\n",
        "num_classes"
      ],
      "metadata": {
        "id": "FWNivQK2DvXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "1y4CtAMESQG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "class BERTModel(nn.Module):\n",
        "  def __init__(self, num_classes, dropout = 0.5):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    modules_to_freeze = [\n",
        "            self.bert.embeddings,\n",
        "            *self.bert.encoder.layer[:8]  # Freeze first 8 layers\n",
        "    ]\n",
        "\n",
        "    for module in modules_to_freeze:\n",
        "      for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(768, 256),\n",
        "        nn.BatchNorm1d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, labels = None):\n",
        "    _, pooled_output = self.bert(\n",
        "     input_ids = input_ids,\n",
        "     attention_mask = attention_mask,\n",
        "     return_dict = False\n",
        "    )\n",
        "\n",
        "    final_layer = self.classifier(pooled_output)\n",
        "    return final_layer"
      ],
      "metadata": {
        "id": "X8KKSFZ0ydDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes, device):\n",
        "  model = BERTModel(num_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # Print parameters stats\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  print(f\"Total parameters: {total_params:,}\")\n",
        "  print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "  print(f\"Percentage trainable: {(trainable_params/total_params)*100:.2f}%\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "r7AEXp4Oz4HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(num_classes, device)"
      ],
      "metadata": {
        "id": "3pSbPtaZ0bsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Train and Test Loop"
      ],
      "metadata": {
        "id": "wY8elISlEtHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        y_true (torch.Tensor): Truth labels for predictions.\n",
        "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
        "\n",
        "    Returns:\n",
        "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
        "    \"\"\"\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ],
      "metadata": {
        "id": "NHLlZDAwFVf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5, weight_decay = 0.01)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "MVilBXcWEl_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode = 'max',\n",
        "    factor = 0.2,\n",
        "    patience = 2,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "_tfIEHhRfsJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              accuracy_fn,\n",
        "              device:torch.device = device):\n",
        "  \"\"\"\n",
        "  Performs train step with the model trying to learn on data_loader\n",
        "  \"\"\"\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.train()\n",
        "\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    # 1. Forward pass\n",
        "    outputs = model(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask = attention_mask\n",
        "    )\n",
        "    # 2. Calculate the loss\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    train_loss += loss.item()\n",
        "    train_acc += accuracy_fn(y_true = labels, y_pred = outputs.argmax(dim = 1))\n",
        "    # 3. optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  # Calculate the general loss and accuracy\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "x4Iz8zrZFaGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch in data_loader:\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "\n",
        "          outputs = model(\n",
        "            input_ids = input_ids,\n",
        "            attention_mask = attention_mask\n",
        "          )\n",
        "\n",
        "          loss = loss_fn(outputs, labels)\n",
        "          test_loss += loss.item()  # Use .item() for scalars\n",
        "          test_acc += accuracy_fn(y_true=labels, y_pred=outputs.argmax(dim=1))\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}\")\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "k8Y1tOZKKX6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float,\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "  \"\"\"\n",
        "  Prints difference between start and endt time.\n",
        "  \"\"\"\n",
        "  total_time = end-start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "nOYTqn1bNzq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "7CIjV_4iNRaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure the time\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch: {epoch}\\n-----\")\n",
        "  train_step(model = model,\n",
        "             data_loader = train_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             accuracy_fn = accuracy_fn)\n",
        "  test_loss, test_acc = test_step(model = model,\n",
        "                        loss_fn = loss_fn,\n",
        "                        data_loader = test_dataloader,\n",
        "                        accuracy_fn = accuracy_fn)\n",
        "\n",
        "  scheduler.step(test_acc)\n",
        "  current_lr = optimizer.param_groups[0]['lr']\n",
        "  print(f\"Current learning rate: {current_lr}\")\n",
        "\n",
        "train_time_end_on_gpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start = train_time_start_on_gpu,\n",
        "                                            end = train_time_end_on_gpu,\n",
        "                                            device = device)"
      ],
      "metadata": {
        "id": "tkUbVCeMNSqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "qcoXp6_iN5rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device = device):\n",
        "  \"\"\"\n",
        "  Returns a dictionary contaning the results of model predicting on data_loader\n",
        "  \"\"\"\n",
        "  test_loss, test_acc = 0, 0\n",
        "  with torch.inference_mode():\n",
        "    for X, y in tqdm(data_loader):\n",
        "      # Make data device agnostic\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # Make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Accumulate the loss and acc values per batch\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      test_loss += loss\n",
        "      test_acc += accuracy_fn(y_true = y,\n",
        "                         y_pred = y_pred.argmax(dim = 1))\n",
        "\n",
        "    # Scale the loss and acc to find the average loss\\acc per batch\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "\n",
        "  return {\"model_name\": model.__class__.__name__, # Only works when model was created with a class\n",
        "          \"model_loss\": test_loss.item(),\n",
        "          \"model_acc\": test_acc}"
      ],
      "metadata": {
        "id": "kCcP69lCN7jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "results = eval_model(model = model,\n",
        "                             data_loader = test_dataloader,\n",
        "                             loss_fn = loss_fn,\n",
        "                             accuracy_fn = accuracy_fn,\n",
        "                             device = device)\n",
        "results"
      ],
      "metadata": {
        "id": "xBKOBqv0N-w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SllouGIqWP6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}